{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers on SciBERT (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = transformers.AutoModelForTokenClassification.from_pretrained('allenai/scibert_scivocab_uncased', num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the data pre-processing pipeline:\n",
    "1. txt files (train, test, dev)\n",
    "2. primary data (X_primary) =  dict of (id: list), (token: list), and (ner_tag: list) for every sentence; grouped by key (not sentence).\n",
    "3. tokenized data (X_tokenized) = 'dict' of token_id, label, and attention_mask; grouped by key.\n",
    "4. Dataset objects (X_data) = a 'list' of token_id, label, and attention mask; grouped by sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output Args\n",
    "DATA_DIR: str = \"./data/ner_chemprot/\"\n",
    "DATA_FILES: dict = {\n",
    "    \"train\": DATA_DIR + 'train.txt', \n",
    "    \"test\": DATA_DIR + 'test.txt', \n",
    "    \"val\": DATA_DIR + 'dev.txt'\n",
    "}\n",
    "label_list = ['O',\n",
    "          'B-enzyme',\n",
    "          'B-SUBSTRATE',\n",
    "          'I-SUBSTRATE',\n",
    "          'B-PRODUCT-OF',\n",
    "          'I-enzyme',\n",
    "          'I-PRODUCT-OF'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1-2 Loading the chemprot data from SciBERT into Primary Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2primary(fname) -> OrderedDict:\n",
    "    # initialize primary data dict\n",
    "    primary_data = OrderedDict()\n",
    "    primary_data['id'] =  []\n",
    "    primary_data['tokens'] = []\n",
    "    primary_data['ner_tags'] = []\n",
    "    \n",
    "#     fname = DATA_DIR + fi #'head.txt' # to test with 2 sentences only.\n",
    "    \n",
    "    sentence_id = 0\n",
    "    with open(fname, \"r\") as f:\n",
    "        rd = csv.reader(f, delimiter='\\t')\n",
    "        \n",
    "        is_blank_after_docstart = False\n",
    "        tmp_words = []\n",
    "        tmp_ners = []\n",
    "        for row in rd:\n",
    "            if is_blank_after_docstart:\n",
    "                is_blank_after_docstart = False\n",
    "                continue\n",
    "            elif not row:\n",
    "                continue\n",
    "            elif re.findall('DOCSTART', row[0]):\n",
    "                is_blank_after_docstart = True\n",
    "                continue\n",
    "            elif row[0] == '.' and row[1] == '.': # currently doesn't include periods.\n",
    "\n",
    "                primary_data['id'].extend([sentence_id])\n",
    "                primary_data['tokens'].extend([tmp_words])\n",
    "                primary_data['ner_tags'].extend([tmp_ners])\n",
    "\n",
    "                \n",
    "                sentence_id += 1\n",
    "                tmp_words = []\n",
    "                tmp_ners = []\n",
    "                continue\n",
    "                \n",
    "            tmp_words += [row[0]]\n",
    "            tmp_ners += [label_list.index(row[3])]\n",
    "    return primary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_primary: OrderedDict = txt2primary(DATA_FILES['train'])\n",
    "val_primary: OrderedDict = txt2primary(DATA_FILES['val'])\n",
    "test_primary: OrderedDict = txt2primary(DATA_FILES['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'tokens': ['The',\n",
       "  'enzyme',\n",
       "  'cyclo-oxygenase',\n",
       "  'catalyses',\n",
       "  'the',\n",
       "  'oxygenation',\n",
       "  'of',\n",
       "  'arachidonic',\n",
       "  'acid',\n",
       "  ',',\n",
       "  'leading',\n",
       "  'to',\n",
       "  'the',\n",
       "  'formation',\n",
       "  'of',\n",
       "  'prostaglandins'],\n",
       " 'ner_tags': [0, 0, 1, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4]}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_entry(i, primary_data: OrderedDict) -> dict:\n",
    "    out = {\n",
    "        'id': primary_data['id'][i],\n",
    "        'tokens': primary_data['tokens'][i],\n",
    "        'ner_tags': primary_data['ner_tags'][i]\n",
    "    }\n",
    "    return out\n",
    "\n",
    "example = get_entry(0, train_primary)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2-3 Tokenize the Primary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenize the primary data to get their encodings, and create a Dataset object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying the tokenizer, based on the reference notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'enzyme', 'cyclo', '-', 'oxygen', '##ase', 'cataly', '##ses', 'the', 'oxygenation', 'of', 'arachid', '##onic', 'acid', ',', 'leading', 'to', 'the', 'formation', 'of', 'prostaglandin', '##s', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 24)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[f\"ner_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"ner_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and apply the tokenization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def tokenize_and_align_labels(primary) -> transformers.tokenization_utils_base.BatchEncoding: # basically dict\n",
    "    tokenized_inputs = tokenizer(primary[\"tokens\"], \n",
    "                                 padding=True, \n",
    "                                 truncation=True, \n",
    "                                 is_split_into_words=True,\n",
    "                                 return_token_type_ids=False\n",
    "                                )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(primary[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenized inputs\n",
    "train_tokenized = tokenize_and_align_labels(train_primary)\n",
    "test_tokenized = tokenize_and_align_labels(test_primary)\n",
    "val_tokenized = tokenize_and_align_labels(val_primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3-4 Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        self.labels = encodings['labels']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = list(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(train_tokenized)\n",
    "test_data = Dataset(test_tokenized)\n",
    "val_data = Dataset(val_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine Tuning + Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of fine tuning and training:\n",
    "1. Metrics function\n",
    "2. Training\n",
    "3. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRODUCT-OF': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'SUBSTRATE': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'enzyme': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "metric = datasets.load_metric(\"seqeval\")\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p) -> dict:\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining necessary functions as args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_init for hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/162 08:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.159183</td>\n",
       "      <td>13.022200</td>\n",
       "      <td>22.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.151794</td>\n",
       "      <td>14.014400</td>\n",
       "      <td>20.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146658</td>\n",
       "      <td>16.007400</td>\n",
       "      <td>18.179000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=162, training_loss=0.14561116253888165, metrics={'train_runtime': 529.7007, 'train_samples_per_second': 0.306, 'total_flos': 246527247372120.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 6837689, 'init_mem_cpu_peaked_delta': 1896582, 'train_mem_cpu_alloc_delta': 318370, 'train_mem_cpu_peaked_delta': 306299})"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    f\"test-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_data,         # training dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForTokenClassification(tokenizer),\n",
    "    eval_dataset=val_data,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    model_init=model_init\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRODUCT-OF': {'precision': 0.125,\n",
       "  'recall': 0.125,\n",
       "  'f1': 0.125,\n",
       "  'number': 32},\n",
       " 'SUBSTRATE': {'precision': 0.5434782608695652,\n",
       "  'recall': 0.20491803278688525,\n",
       "  'f1': 0.2976190476190476,\n",
       "  'number': 122},\n",
       " 'enzyme': {'precision': 0.4067796610169492,\n",
       "  'recall': 0.24615384615384617,\n",
       "  'f1': 0.30670926517571884,\n",
       "  'number': 195},\n",
       " 'overall_precision': 0.39285714285714285,\n",
       " 'overall_recall': 0.22063037249283668,\n",
       " 'overall_f1': 0.28256880733944956,\n",
       " 'overall_accuracy': 0.9552918741392097}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(predictions[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Hyperparameter search??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "To use hyperparameter search, you need to pass your model through a model_init function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-9587b6060600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/trainer.py\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             raise RuntimeError(\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;34m\"To use hyperparameter search, you need to pass your model through a model_init function.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m             )\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: To use hyperparameter search, you need to pass your model through a model_init function."
     ]
    }
   ],
   "source": [
    "trainer.hyperparameter_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# torch.manual_seed(10)\n",
    "# BATCH_SIZE = 64\n",
    "# train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# optim = transformers.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# for epoch in range(3):\n",
    "#     for i, batch in enumerate(train_loader):\n",
    "#         print(f'Doing epoch {epoch}, entries {i*BATCH_SIZE} to {(i+1)*BATCH_SIZE} out of {len(train_loader)}')\n",
    "#         optim.zero_grad()\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs[0]\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 (transformers)",
   "language": "python",
   "name": "python3.6.13_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
